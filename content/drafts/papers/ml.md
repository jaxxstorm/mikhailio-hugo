## Optimizing Prediction Serving on Low-Latency Serverless Dataflow

Cloudflow: ML model serving for interactive applications built on top of some stateful FaaS research platform Cloudburst (https://arxiv.org/pdf/2001.04592.pdf)

https://arxiv.org/pdf/2007.05832v1.pdf

## Serverless inferencing on Kubernetes

Using KFServing to serve ML inference models on Kubernetes. Built on top of KNative and optimizes things like GPU-VMs scaling.

https://arxiv.org/pdf/2007.07366v1.pdf